= Advanced Installation
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:prewrap!:

toc::[]

== Overview
For production environments, a reference configuration implemented using
http://www.ansible.com[Ansible] playbooks is available as the _advanced
installation_ method for installing OpenShift hosts. Familiarity with Ansible is
assumed, however you can use this configuration as a reference to create your
own implementation using the configuration management tool of your choosing.

Alternatively, you can use the link:quick_install.html[quick installation]
method for trial installations.

== Prerequisites

Before installing OpenShift, you must first link:prerequisites.html[satisfy the
prerequisites] on your hosts, which includes verifying system and environment
requirements per component type and properly installing and configuring Docker.

After following the
instructions in the link:prerequisites.html[Prerequisites] topic, you can
continue to link:#installing-ansible[installing Ansible].

[[installing-ansible]]

== Installing Ansible

The advanced installation method is based on Ansible playbooks and as such
requires invoking Ansible directly. Confirm your organization's supported
download instructions for Ansible with your IT department.

For example, Ansible is available in the EPEL repository. To use this package
source, install the EPEL repository:

----
# yum -y install \
    https://dl.fedoraproject.org/pub/epel/7/x86_64/e/epel-release-7-5.noarch.rpm
----

Disable the EPEL repository globally so that it is not accidentally used during
later steps of the installation:

----
# sed -i -e "s/^enabled=1/enabled=0/" /etc/yum.repos.d/epel.repo
----

Install the packages for Ansible:

----
# yum -y --enablerepo=epel install ansible
----

[[cloning-the-ansible-repository]]

== Cloning the Ansible Repository

The configuration files for the Ansible installer are currently available
https://github.com/openshift/openshift-ansible[on Github]. Clone the
repository:

----
# cd ~
# git clone https://github.com/openshift/openshift-ansible
# cd openshift-ansible
----

[NOTE]
====
Be sure to stay on the *master* branch of the *openshift-ansible* repository for
the remaining instructions.
====

[[configuring-ansible]]

== Configuring Ansible

You must create an *_/etc/ansible/hosts_* file, which is Ansible's inventory
file for the playbook to use during the installation.

Any hosts you designate as masters during the installation process should also
be configured as
link:../../admin_guide/manage_nodes.html#marking-nodes-as-unschedulable-or-schedulable[unschedulable]
nodes. This is so the masters are configured as part of the
link:../../architecture/additional_concepts/networking.html#openshift-sdn[OpenShift
SDN].

The following sections describe example inventory files for various environment
topographies, including configuring
link:../../architecture/infrastructure_components/kubernetes_infrastructure.html#high-availability-masters[multiple
masters for high availability]. You can choose an example that matches your
requirements to modify and use as your inventory file when
link:#running-the-ansible-installer[running the Ansible installer].

[[single-master-multi-node]]

=== Single Master and Multiple Nodes

The following table describes an example environment for a single
link:../../architecture/infrastructure_components/kubernetes_infrastructure.html#master[master]
and two
link:../../architecture/infrastructure_components/kubernetes_infrastructure.html#node[nodes]:

[options="header"]
|===

|Host Name |Infrastructure Component to Install

|*master.example.com*
|Master and node

|*node1.example.com*
.2+.^|Node

|*node2.example.com*
|===

You can see these example hosts present in the *[masters]* and *[nodes]*
sections of the following example inventory file:

.Single Master and Multiple Nodes Inventory File
====

----
# Create an OSEv3 group that contains the masters and nodes groups
[OSEv3:children]
masters
nodes

# Set variables common for all OSEv3 hosts
[OSEv3:vars]
# SSH user, this user should allow ssh based auth without requiring a password
ansible_ssh_user=root

# If ansible_ssh_user is not root, ansible_sudo must be set to true
#ansible_sudo=true

product_type=openshift
ifdef::openshift-enterprise[]
deployment_type=enterprise
endif::[]
ifdef::openshift-origin[]
deployment_type=origin
endif::[]

# uncomment the following to enable htpasswd authentication; defaults to DenyAllPasswordIdentityProvider
#openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider', 'filename': '/etc/openshift/openshift-passwd'}]

# host group for masters
[masters]
master.example.com

# host group for nodes, includes region info
[nodes]
master.example.com openshift_node_labels="{'region': 'infra', 'zone': 'default'}"
node1.example.com openshift_node_labels="{'region': 'primary', 'zone': 'east'}"
node2.example.com openshift_node_labels="{'region': 'primary', 'zone': 'west'}"
----
====

Modify the file to match your environment and specifications, and save it as
*_/etc/ansible/hosts_*.

[[single-master-multi-etcd-multi-node]]

=== Single Master, Multiple etcd, and Multiple Nodes

The following table describes an example environment for a single
link:../../architecture/infrastructure_components/kubernetes_infrastructure.html#master[master],
three
link:../../architecture/infrastructure_components/kubernetes_infrastructure.html#master[*etcd*]
hosts, and two
link:../../architecture/infrastructure_components/kubernetes_infrastructure.html#node[nodes]:

[options="header"]
|===

|Host Name |Infrastructure Component to Install

|*master.example.com*
|Master and node

|*etcd1.example.com*
.3+.^|etcd

|*etcd2.example.com*

|*etcd3.example.com*

|*node1.example.com*
.2+.^|Node

|*node2.example.com*
|===

[NOTE]
====
When specifying multiple *etcd* hosts, external *etcd* is installed and
configured. Clustering of OpenShift's embedded *etcd* is not supported.
====

You can see these example hosts present in the *[masters]*, *[nodes]*, and
*[etcd]* sections of the following example inventory file:

.Single Master, Multiple etcd, and Multiple Nodes Inventory File
====

----
# Create an OSEv3 group that contains the masters and nodes groups
[OSEv3:children]
masters
nodes
etcd

# Set variables common for all OSEv3 hosts
[OSEv3:vars]
ansible_ssh_user=root
product_type=openshift
ifdef::openshift-enterprise[]
deployment_type=enterprise
endif::[]
ifdef::openshift-origin[]
deployment_type=origin
endif::[]

# uncomment the following to enable htpasswd authentication; defaults to DenyAllPasswordIdentityProvider
#openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider', 'filename': '/etc/openshift/openshift-passwd'}]

# host group for masters
[masters]
master.example.com

# host group for etcd
[etcd]
etcd1.example.com
etcd2.example.com
etcd3.example.com

# host group for nodes, includes region info
[nodes]
master.example.com openshift_node_labels="{'region': 'infra', 'zone': 'default'}"
node1.example.com openshift_node_labels="{'region': 'primary', 'zone': 'east'}"
node2.example.com openshift_node_labels="{'region': 'primary', 'zone': 'west'}"
----
====

Modify the file to match your environment and specifications, and save it as
*_/etc/ansible/hosts_*.

[[multi-master-multi-etcd-multi-node]]

=== Multiple Masters, Multiple etcd, and Multiple Nodes

The following describes an example environment for three
link:../../architecture/infrastructure_components/kubernetes_infrastructure.html#master[masters],
three
link:../../architecture/infrastructure_components/kubernetes_infrastructure.html#master[*etcd*]
hosts, and two
link:../../architecture/infrastructure_components/kubernetes_infrastructure.html#node[nodes]:

[options="header"]
|===

|Host Name |Infrastructure Component to Install

|*master1.example.com*
.3+.^|Master
(link:../../architecture/infrastructure_components/kubernetes_infrastructure.html#high-availability-masters[clustered
using Pacemaker]) and node

|*master2.example.com*

|*master3.example.com*

|*etcd1.example.com*
.3+.^|etcd

|*etcd2.example.com*

|*etcd3.example.com*

|*node1.example.com*
.2+.^|Node

|*node2.example.com*
|===

[NOTE]
====
When specifying multiple *etcd* hosts, external *etcd* is installed and
configured. Clustering of OpenShift's embedded *etcd* is not supported.
====

You can see these example hosts present in the *[masters]*, *[nodes]*, and
*[etcd]* sections of the following example inventory file:

.Multiple Masters, Multiple etcd, and Multiple Nodes Inventory File
====

----
# Create an OSEv3 group that contains the masters and nodes groups
[OSEv3:children]
masters
nodes
etcd

# Set variables common for all OSEv3 hosts
[OSEv3:vars]
ansible_ssh_user=root
product_type=openshift
ifdef::openshift-enterprise[]
deployment_type=enterprise
endif::[]
ifdef::openshift-origin[]
deployment_type=origin
endif::[]

# uncomment the following to enable htpasswd authentication; defaults to DenyAllPasswordIdentityProvider
# openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider', 'filename': '/etc/openshift/openshift-passwd'}]

# master cluster ha variables using pacemaker or RHEL HA
openshift_master_cluster_password=openshift_cluster
openshift_master_cluster_vip=192.168.133.25
openshift_master_cluster_public_vip=192.168.133.25
openshift_master_cluster_hostname=openshift-master.example.com
openshift_master_cluster_public_hostname=openshift-master.example.com


# host group for masters
[masters]
master1.example.com
master2.example.com
master3.example.com

# host group for etcd
[etcd]
etcd1.example.com
etcd2.example.com
etcd3.example.com

# host group for nodes, includes region info
[nodes]
master[1:3].example.com openshift_node_labels="{'region': 'infra', 'zone': 'default'}"
node1.example.com openshift_node_labels="{'region': 'primary', 'zone': 'east'}"
node2.example.com openshift_node_labels="{'region': 'primary', 'zone': 'west'}"
----
====

Modify the file to match your environment and specifications, and save it as
*_/etc/ansible/hosts_*.

Note the following when using this configuration:

- Installing multiple masters requires that you
link:#configuring-fencing[configure a fencing device] after running the
installer.
- When specifying multiple masters, the installer handles creating and starting
the high availability (HA) cluster. If during that process the `pcs status`
command indicates that an HA cluster already exists, the installer skips HA
cluster configuration.

== Running the Ansible Installer

You can now run the Ansible installer:

----
# ansible-playbook ~/openshift-ansible/playbooks/byo/config.yml
----

If for any reason the installation fails, before re-running the installer, see
link:#installer-known-issues[Known Issues] to check for any specific
instructions or workarounds.


[[configuring-fencing]]

== Configuring Fencing

If you installed OpenShift using a
link:#multi-master-multi-etcd-multi-node[configuration for multiple masters],
you must configure a fencing device. See
https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/High_Availability_Add-On_Reference/ch-fencing-HAAR.html[Fencing:
Configuring STONITH] in the High Availability Add-on for Red Hat Enterprise
Linux documentation for instructions, then continue to
link:#verifying-the-installation[Verifying the Installation].

[[verifying-the-installation]]

== Verifying the Installation

After the installer completes, you can verify that the master is started and
nodes are registered and reporting in *Ready* status by running the following as
*root*:

====
----
# oc get nodes

NAME                      LABELS                                                                     STATUS
master.example.com        kubernetes.io/hostname=master.example.com,region=infra,zone=default        Ready,SchedulingDisabled
node1.example.com         kubernetes.io/hostname=node1.example.com,region=primary,zone=east          Ready
node2.example.com         kubernetes.io/hostname=node2.example.com,region=primary,zone=west          Ready
----
====

*Multiple etcd Hosts*

If you installed multiple *etcd* hosts:

. On a master host, verify the *etcd* cluster health, substituting for the FQDNs
of your *etcd* hosts in the following:
+
====
----
# etcdctl -C \
    https://etcd1.example.com:2379,https://etcd2.example.com:2379,https://etcd3.example.com:2379 \
    --ca-file=/etc/openshift/master/master.etcd-ca.crt \
    --cert-file=/etc/openshift/master/master.etcd-client.crt \
    --key-file=/etc/openshift/master/master.etcd-client.key cluster-health
----
====

. Also verify the member list is correct:
+
====
----
# etcdctl -C \
    https://etcd1.example.com:2379,https://etcd2.example.com:2379,https://etcd3.example.com:2379 \
    --ca-file=/etc/openshift/master/master.etcd-ca.crt \
    --cert-file=/etc/openshift/master/master.etcd-client.crt \
    --key-file=/etc/openshift/master/master.etcd-client.key member list
----
====

*Multiple Masters*

If you installed multiple masters:

. On a master host, determine which host is currently running as the active
master:
+
----
# pcs status
----

. After determining the active master, put the specified host into standby mode:
+
----
# pcs cluster standby <host1_name>
----

. Verify the master is now running on another host:
+
----
# pcs status
----

. After verifying the master is running on another node, re-enable the host on standby for normal operation by running:
+
----
# pcs cluster unstandby <host1_name>
----

Red Hat recommends that you also verify your installation by consulting the
https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html-single/High_Availability_Add-On_Reference/index.html[High
Availability Add-on for Red Hat Enterprise Linux documentation].

[[installer-known-issues]]

== Known Issues

The following are known issues for specified installation configurations.

*Multiple Masters*

- On failover, it is possible for the controller manager to overcorrect, which
causes the system to run more pods than what was intended. However, this is a
transient event and the system does correct itself over time. See
https://github.com/GoogleCloudPlatform/kubernetes/issues/10030 for details.

- On failure of the Ansible installer, you must start from a clean operating
system installation. If you are using virtual machines, start from a fresh
image. If you are use bare metal machines:
+
. Run the following on a master host:
+
----
# pcs cluster destroy --all
----
+
. Then, run the following on all node hosts:
+
----
# yum -y remove openshift openshift-* etcd docker

# rm -rf /etc/openshift /var/lib/openshift /etc/etcd \
    /var/lib/etcd /etc/sysconfig/openshift* /etc/sysconfig/docker* \
    /root/.kube/config /etc/ansible/facts.d /usr/share/openshift
----

== What's Next?

Now that you have a working OpenShift instance, you can:

- link:../configuring_authentication.html[Configure authentication]; by default,
authentication is set to
link:../configuring_authentication.html#DenyAllPasswordIdentityProvider[Deny
All].
- Deploy an link:docker_registry.html[integrated Docker registry].
- Deploy a link:deploy_router.html[router].
- link:first_steps.html[Populate your OpenShift installation] with a useful set
of Red Hat-provided image streams and templates.
